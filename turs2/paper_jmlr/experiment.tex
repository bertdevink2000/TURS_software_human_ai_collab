\input{data_info_table.tex} 
\section{Experiments} \label{sec:exp}
\subsection{Setup}
\textbf{Datasets.} We conduct an extensive experiments with 31 datasets, summarized in Table~\ref{table:data_info}. Our multi-class datasets are from the UCI repository, while the binary-class datasets are from both the UCI repository and the ADBench Github repository \citep{han2022adbench}. The latter is a benchmark toolbox for anomaly detection (including imbalanced classification), hence we include it to have both balanced and imbalanced binary datasets.

\noindent
\textbf{Competitors.} We compare against a wide ranges of methods: 1) unordered CN2, the one-versus-rest rule sets method without implicit order among rules; 2) DRS, a representative multi-class rule set method which aims for minimizing the size of overlaps; 3) IDS, the multi-class rule set method optimizing a linear combination of scores, which characterize the quality of rules in seven perspectives, 4) RIPPER, the widely used one-versus-rest method with orders among class labels, 5) CLASSY, the probabilistic rule list methods using MDL-based model selection; 6) CART, the well-known decision tree method, with post-pruning by a validation dataset separated from the training set; 7) C4.5 decision tree, with post-pruning by the MDL principle; 8) BRS, the bayesian rule sets method (only for binary dataset). 

\noindent{\textbf{Algorithm configurations.}} For TURS, we set the beam width as 10, and the number of candidate cut points for numeric features as 20. For competitor algorithms, we use CN2 from Orange~\citep{JMLR:demsar13a}, IDS from a third-party implementation with proved scalability \citep{filip2019pyids}, RIPPER and C4.5 from Weka~\citep{hall2009weka} and its R wrapper, CART from Python's Scikit-Learn package~\citep{scikit-learn}, and finally, DRS, BRS, CLASSY from the original authors. Competitors algorithms' configurations are set to be the same as the default as in the paper and/or in original authors' implementations. We make the code public for reproducibility, including the code for competitor algorithms\footnote{\url{...}}.


\input{table_roc_auc}

\begin{figure}[ht]
\includegraphics[width=\textwidth]{boxplot_roc_auc}	
\caption{For each algorithm, we calculate for every individual dataset the difference between its ROC-AUC score and the best ROC-AUC scores. The differences to the best ROC-AUC scores for each algorithm is illustrated by a box-plot.} \label{fig:diff_to_best_auc}
\end{figure}

\subsection{Classification performance} \label{subsec:classifier_perf}
To investigate the classification performance for TURS, we report  in Table~\ref{table:roc_auc} the ROC-AUC scores on the test sets, averaged over five-fold cross-validations. For multi-class classification, we report the ``macro" one-versus-rest AUC scores, as ``macro" AUC treats all class labels equally and hence can characterize how well the classifiers predict for the minority classes. 

Note that BRS~\citep{wang2017bayesian} can only be applied to binary datasets. Further, we fail to obtain the results of DRS on three datasets because the implementation of DRS does not support the large number of columns (it simply gives an error). We also fail to obtain the results of IDS on several datasets as they exceed the predetermined time limit: 10 hours for one single fold of one dataset. 

We have shown that TURS is very competitive in comparison to its competitors in the following aspects. First, TURS performs the best in 11 out of the total 31 datasets, and performs the best in 6 out of 11 multi-class datasets. We denote the best ROC-AUC for each dataset in bold. 

Second, we report the difference between TURS's ROC-AUC scores and the best ROC-AUC scores for each individual dataset, in the bracket in the table. We also repeat this process for all competitor algorithms, obtaining their ``gaps-to-best" scores. We further demonstrate these gaps-to-best scores in Figure~\ref{fig:diff_to_best_auc}. The box-plots demonstrate that TURS is very stable for all 31 datasets we have tested: in comparison to its competitors, the gaps-to-best scores are much smaller. 

Third, among all rule sets methods (CN2, DRS, IDS, TURS), TURS show substantially superior performance against DRS and IDS. As DRS and IDS both aim to reduce the sizes of overlaps, our results indicate that minimizing the sizes of overlaps may impose a too restricted constraint and hence lead to sub-optimal classification performance. On the other hand, CN2 is competitive in terms of obtaining the best AUCs, especially for binary datasets, as shown in Table~\ref{table:roc_auc}. However,  as shown in Figure~\ref{fig:diff_to_best_auc}, CN2 has in general larger gaps to the best AUCs than TURS does. Further, more comparisons illustrating TURS superiorities over CN2 will be in the following paragraphs. 

\input{table_rp_roc_auc}
\begin{figure}[ht] \centering
\includegraphics[width=0.75\textwidth]{boxplot_random_pick}	
\caption{Box-plots for ROC-AUC minus ROC-AUC with ``random picking", for TURS and CN2, averaged over the five-fold stratified cross-validations. } \label{fig:diff_rp}
\end{figure}

\subsection{Prediction with `random picking' for overlaps}
Recall that we estimate the class probabilities for overlaps by considering the ``union" of the cover of all involved rules. Thus, the next question we study empirically is whether our formalization of rule sets as probabilistic models can indeed lead to overlaps only formed by rules with similar probabilistic estimations. 

Therefore, we compare the (probabilistic) predictions of our TURS models against the probabilistic predictions by what we call ``random picking" for overlaps: when an unseen instance is covered by multiple rules, we randomly pick one individual rule, and use its estimated class probabilities (estimated from training set) as the probabilistic prediction for this instance. 

Intuitively, if the overlaps are formed only by rules with similar probabilistic output, we expect the probabilistic prediction performance by TURS and by TURS with random-picking (TURS-RP) to be very close. We first report the ROC-AUC of TURS and TURS-RP in Table~\ref{table:roc_auc_rp}, together with the percentage of instances covered by more than one rules (\%~overlaps), which we also benchmark against CN2 (IDS and DRS are excluded due to their sub-optimal performance in general). We observe that the ROC-AUC for TURS and TURS-RP is almost the same, while the ROC-AUC for CN2 and CN2 with random picking (CN2-RP) can differ substantially in general, as also summarized in Figure~\ref{fig:diff_rp}.
\begin{figure}[ht] \centering 
	\includegraphics[width=\textwidth]{logloss_randompicking}
	\caption{Log-loss of TURS, with and without the ``random picking", averaged over the stratified five-fold cross-validations. } \label{fig:logloss_rp}
\end{figure}

Further, we show in Figure~\ref{fig:logloss_rp} the log-loss of TURS and TURS-RP, to investigate the effect of the ``random picking" on the predictive class probabilities. We observe that, except for a few exceptions, the difference between the two log-losses for all datasets are minimal. Specifically, the few exceptions can be explained by 1) small sample sizes, including the dataset ``glass" (sample size 214), and ``vehicle" (sample size 846); 2) very large percentage of instances covered by multiple rules and including the dataset ``drybeans"~(34\%) and ``pendigits"~(40\%). 

We can hence conclude that, while CN2 relies heavily on its conflict resolving schemes for overlaps, TURS produces overlaps only formed by decision rules with very similar probability estimates. This indicates our decision rules can be viewed as \emph{truly unordered in the sense that, when an instance is covered by multiple rules, it has very little effect on which rule is picked to predict its class probabilities.} In other words, overlaps in our model provides a multi-perspectives descriptions to the corresponding instances; in contrast, all existing methods treat overlaps as ``nuisances". 

\begin{figure}[ht]\centering
\includegraphics[width=\textwidth]{train_test_diff_rulesets}
\includegraphics[width=\textwidth]{train_test_diff_treeandlist}
\caption{The weighted average of the differences between the class probability estimates of every individual rule for training and test sets, in which the weights the coverage of each rule (for the training set). The figures report the average on five-fold stratified cross-validations: the above figure shows the comparison against rule sets competitor methods, while the below one against decision list and tree methods.}
\label{fig:train_test_diff}
\end{figure}
\subsection{Generalizability of local probabilistic estimates}
While decision rules are widely accepted as intrinsically explainable models, rules with probability estimates that generalize well can serve as good explanations. Thus, we next examine the difference between individual rules' probability estimates on the train and test sets, with five-fold stratified cross-validations. 

Specifically, given a ruleset induced from a specific dataset, we look at each individual rule's probability estimates, estimated from the training and test set respectively, by the maximum likelihood estimator (MLE). We next take the average of the class probabilities (both for binary and multi-class targets). Finally, we report the weighted average for all rules, weighted by the coverage of each rule (on the training set). 

Formally, given a ruleset with $K$ rules: $M = \{S_1, ..., S_K\}$, denote the probability estimates of all rules by $(\mathbf{p}_1, ..., \mathbf{p}_K)$ and $(\mathbf{q}_1, ..., \mathbf{q}_K)$, respectively estimated from the training and test set. We measure how well the individual rules generalize by
\begin{equation} \label{eq:train_test_diff}
	g = \frac{1}{K}\sum_{j} |S_i| (\bar{\mathbf{p}_i} - \bar{\mathbf{q}_i}),
\end{equation}
in which $\bar{\mathbf{p}_i}$ and $\bar{\mathbf{q}_i}$ denote the mean of elements of the estimated class probability vectors. Note that each individual rule is treated separately in calculating the $g$-score above, and hence the overlaps do not play a role here. 

We calculate this score for all algorithms and all datasets, averaged over five-fold cross-validations, and demonstrate the results in Figure~\ref{fig:train_test_diff}. As a result, we observe that TURS (the bold purple curve) mostly lies at the lowest level, and is much more stable than all competitor algorithms. 

The only exception is IDS (blue line in the upper figure): while the rules' probability estimates of IDS generalize better than that of TURS in some datasets, this indicates IDS has serious ``under-fitting" if we take into consideration IDS's suboptimal predictive performance as discussed in Section~\ref{subsec:classifier_perf}. That is, IDS produces rules with too large coverage, and hence is not specific and refined enough for classification, although rules with large coverage have probability estimates that generalize well. 

%We also report the exact numbers for the score calculated by Equation~\ref{eq:train_test_diff} in Table~\ref{table:train_test_diff}.

Thus, in conclusion, rules induced by TURS serve better as explanations for the predictions of the tree- and rule-based models, in terms of the quality of its probability estimates. 

%\input{table_traintestdiff}

\input{table_model_complexity2}
\begin{figure}[ht]
	\includegraphics[width=\textwidth]{heatmap_model_complexity}
	\caption{Heatmap for model complexity (higher is better): for each dataset, we divide the best (minimum) total number of literals by the total number of literals of each algorithm. We exclude the results from the models with much worse ROC-AUC scores (denoted as grey).}	 
	\label{fig:heatmap_modelcomplexity}
\end{figure}

\subsection{Model complexities}
The next question we study empirically is, does TURS produce more complex rule sets because it only allows overlaps formed by rules with similar probabilistic output? 

We first calculate the number of total literals for each model, by summing up the lengths of rules in a rule set, rule list, or decision tree (by treating each tree path as rule). 
%The total number of literals in a model represents the complexity if a domain expert would like to comprehend the model as a whole. 
We report this measure in Table~\ref{table:num_literal}. As simpler models with very much worse predictive performance is not so interesting, hence if a model's ROC-AUC score is more than $(-0.1)$ smaller than that of TURS, we exclude it from the following comparisons (denoted by \emph{smaller} font sizes in Table~\ref{table:num_literal}). 

We observe that TURS produces the simplest model for 13 out 31 datasets (excluding results from models with incomparable ROC-AUC scores), which we denote in bold in the table. 

Further, to illustrate the differences between the number of literals across all algorithms, we calculate a comparative score as follow: for each individual dataset, we divide the minimum total number of literals by the total number of literals of each algorithm, which we plot as a heat map in Figure~\ref{fig:heatmap_modelcomplexity}. From the heat map, CLASSY and TURS are obviously better than others in general; however, CLASSY induces ordered decision lists from data, with a much more complex internal logic than the unordered rule sets induced by TURS. 

%\textbf{Rule lengths.} For rule-based models with a large number of total literals, it is impractical for domain experts or analysts to comprehend the whole model by reading through all rules. In such cases, they may be more interesting to look at explanations for single predictions; thus, shorter rules are preferred. 
%\todo[inline]{What is the message for rule lengths? Maybe do not report this?}

\subsection{Ablation study: local constraint}
\begin{table}[ht]
\small\centering\begin{tabular}{llllll}  \hlineConstraint & \# rules & rule length & ROC-AUC & MDL score & train/test prob. diff. \\   \hlineNo & 12.48($\pm$1.56) & 5.597($\pm$0.42) & 0.722($\pm$0.02) & 2191.189($\pm$65.91) & 0.049($\pm$0.01) \\   Yes & 1($\pm$0) & 1($\pm$0) & 0.724($\pm$0.01) & 2050.087($\pm$68.88) & 0.007($\pm$0) \\    \hline\end{tabular}
\caption{Results of ablation study on whether to use the local constraint. We report the mean ($\pm$ standard deviation) over $100$ repetitions. } \label{table:ablation_simu}\end{table}
\begin{figure}[ht]
	\includegraphics[width=\textwidth]{ablation_local_constraint}
	\caption{The process of rules being added to the rule set, with and without the local constraint heuristics, using the first  dataset among the 100 simulated datasets. Each point represent the status when after a single rule is added, with x-axis representing the coverage of the (potentially incomplete) rule set after adding this rule, and y-axis representing the MDL-based score. } \label{fig:ablation_simulation}
\end{figure}
We consider a simple simulation study to illustrate the necessity of the local constraint, where our feature variables are denoted as $X = (X_1, ..., X_{50})$. Assume all variables in $X$ are binary, and we sample $X_1 \sim Ber(0.2)$, $X_i \sim Ber(0.5) (i = 2, ..., 50)$, in which $Ber(.)$ denotes the Bernoulli distribution. Further, we consider binary target variable $Y$ and sample $Y|X_1 = 1 \sim Ber(0.7)$ and $Y|X_1 = 0 \sim Ber(0.95)$. That is, $X_1 = 1$ (or $X_1 = 0$) is the only ``rule" in this simulated dataset. 

We simulate the dataset with the sample size $5,000$ for $100$ times, and run TURS with and without the local constraints. As shown in Table~\ref{table:ablation_simu}, without the local constraint, we will achieve worse (bigger) MDL-based score. 

Notably, when not using the local constraint, the number of rules  and the rule lengths are both not consistent with the ``true" model, showing that irrelevant variables are picked when growing the rules. We have two perspectives to explain the inconsistency. 

To begin with, when the local constraint is \emph{not} imposed, the difference between the class probabilities estimated from the training and test dataset is larger than the difference when the local constraint is imposed, showing that the rules as local probabilistic models generalize worse when the local constraint heuristic is turned off. That is, we observe overfitting locally. Further, as we write as motivation in Section~\ref{subsec:local_constraint}, the local constraint heuristic is designed to prevent leaving out instances that are difficult to cover for `future' rules, we do notice this phenomenon empirically. Specifically, for a single run of TURS on the simulated dataset, we plot the process of rules being added to the (potentially incomplete) rule set, with x-axis showing the coverage of the rule set, versus the MDL-based model selection criterion for the rule set. 

We plot in Figure~\ref{fig:ablation_simulation} the procedure of iteratively searching for the next best rule: each point represents the status of the rule set after a single rule is added, with x-axis representing the coverage of the rule set (i.e., the number of instances covered by at least one of the rules), and y-axis representing the MDL-based score for the rule set as a whole model. Thus, our compression learning rate heuristic, defined in Section~\ref{subsec:learning_rate}, basically tries to iteratively find the next point (i.e., next rule) in Figure~\ref{fig:ablation_simulation} with the steepest slope. However, without the local constraint heuristic, the search for the next rule conducted by the algorithm may become too greedy: when growing a rule and consequently reducing the rule's coverage, the instances left out to be covered by future rules are simply ignored, which leads to inferior optimization results in the end (as shown by the red curve in Figure~\ref{fig:ablation_simulation}). 

\subsection{Ablation study: diverse coverage beam search}
\begin{figure}[ht] \label{fig:diverse_coverage}
	\includegraphics[width=\textwidth]{ablation_diverse_coverage}
	\caption{Ablation study: the ROC AUC on the test set, averaged over the five-fold stratified cross-validation.}
	\label{fig:heatmap_modelcomplexity}
\end{figure}
We study the effect of using the beam search with the ``diverse coverage", by replacing it with the ``normal" beam search. Suppose the beam width is $W$, we simply pick the best $W$ rule growth candidates \emph{without categorizing rule growth candidates by their coverage}. That is, we ``turn off" the diverse coverage constraints both for the updating the beam and the auxiliary beam. 

As shown in Figure~\ref{fig:diverse_coverage}, when using the diverse coverage heuristic, the ROC-AUC on the test sets (points and curve in orange) becomes better on 25 out of 31 datasets, demonstrating the benefits for the predictive performance. 

\subsection{Runtime}
\begin{figure}[ht] \label{fig:runtime}
	\includegraphics[width=\textwidth]{runtime}
	\caption{Runtime comparison among rule set methods.}	 
	\label{fig:heatmap_modelcomplexity}
\end{figure}
Last, we report the runtime of TURS, together with all rule set competitor methods only, as decision trees/lists methods from mature software (Weka and Python Scikit-Learn) are highly optimized in speed and are known to be very fast. 

We illustrate the runtime (in seconds), averaged over the five-fold cross-validations, in Figure~\ref{fig:runtime} (note that the y-axis is in scaled by $log_{10}$). In general, the runtime of TURS are competitive among all rule set methods, with only CN2 showing clear superiority. CN2 seems faster in general and scales better to larger datasets, which can be caused both by a more efficient implementation (from the software ``Orange"), and by its algorithmic properties (a greedy and separate-and-conquer approach). 

%In addition, for 22 out of 31 datasets, the runtime of TURS (in purple) are less than 100 seconds. On the other hand, TURS needs more than $10,000$ seconds on 6 of them (backdoor, magic, avila, drybeans, pendigits, waveform). 








%\subsection{Overlaps' insignificance}
%\begin{figure}
%	\includegraphics[width=\textwidth]{overlap_sig}
%\end{figure}





