\section{Related Work} \label{sec:related}

\noindent
\emph{Rule lists.}
%Rules in a rule list are connected by \textsc{if-then-else} statements. Existing methods include ordered-CN2~\citep{clark1989cn2}, CBA~\citep{liu1998CBA}, and PART~\citep{frank1998generating}, as well as the recently proposed CLASSY~\citep{proencca2020interpretable} and Bayesian rule list~\citep{yang2017scalable-bayesian-rule-list}. Rule lists are more difficult to interpret than rule sets because of their explicit orders. 
Rules in a rule list are connected by \textsc{if-then-else} statements. Existing methods include CBA~\citep{liu1998CBA}, ordered CN2~\citep{clark1989cn2}, PART~\citep{frank1998generating}, and the recently proposed CLASSY~\citep{proencca2020interpretable} and Bayesian rule list~\citep{yang2017scalable-bayesian-rule-list}. We argue that rule lists are more difficult to interpret than rule sets because of their explicit orders. 

\smallskip \noindent
\emph{One-versus-rest learning.}
This category focuses on only learning rules for a single class label, i.e., the ``positive" class, which is already sufficient for binary classification \citep{wang2017bayesian,dash2018boolean,yang2021learning}. For multi-class classification, two approaches exist. The first, taken by RIPPER~\citep{cohen1995ripper} and C4.5~\citep{quinlan2014c4}, is to learn each class in a certain order. After all rules for a single class have been learned, all covered instances are removed (or those with this class label). The resulting model is essentially an ordered list of rule sets, and hence is more difficult to interpret than rule set.

The second approach does not impose an order among the classes; instead, it learns a set of rules for each class against all other classes. The most well-known are unordered-CN2 and FURIA~\citep{clark1991cn2Improve,huhn2009furia}. 
%However, they do not learn truly unordered rule sets, as discussed in Section~\ref{sec:intro}. 
FURIA avoids dealing with conflicts of overlaps by using all rules for predicting unseen instances; as a result, it cannot provide a single rule to explain its prediction. Unordered-CN2, on the other hand, handles overlaps by ``combining" all overlapping rules into a ``hypothetical" rule, which sums up all instances in all overlapping rules and hence ignoring probabilistic conflicts for constructing rules. In Section~\ref{sec:exp}, we show that our method learns smaller rule sets with better predictive performance than unordered-CN2.

\smallskip \noindent
\emph{Multi-class rule sets.}
Very few methods exist for directly learning rules for multi-class targets, which is algorithmically more challenging than the one-versus-rest paradigm, as the separate-and-conquer strategy is not applicable. 
To the best of our knowledge, the only existing methods are IDS~\citep{lakkaraju2016interpretable} and DRS~\citep{zhang2020diverseRuleSets}. Both are neither probabilistic nor truly unordered. To handle conflicts of overlaps, IDS follows the rule with the highest F1-score, and DRS uses the most accurate rule. 
%Two existing methods IDS~\citep{lakkaraju2016interpretable} and DRS~\citep{zhang2020diverseRuleSets} are neither probabilistic, nor truly unordered: to handle predictive conflicts of overlaps, IDS follows the rule with the highest F1-score, and DRS uses the most accurate rule. 
%Decision tree methods like CART~\citep{breiman1984classification} might also fall in this category as well, but rules based on decision trees are forced to share many ``attributes", and hence become longer than necessary. 

Last, different but related approaches include 1) decision tree based methods such as CART~\citep{breiman1984classification}, which produce rules that are forced to share many ``attributes" and hence are longer than necessary, as we will empirically demonstrate in Section~\ref{sec:exp}, and 2) a Bayesian rule mining~\citep{gay2012bayesian} method, which adopts naive bayes  with the mined rules for prediction, and hence does not produce a rule set model in the end. The `lazy learning' approach for rule-based models can also avoid the conflicts of overlaps~\citep{veloso2006lazy}, but no global rule set model describing the whole dataset is constructed in this case.

%\smallskip \noindent
%\emph{Bayesian and MDL rule learning.} Last, we mention previous rule learning methods that also adopts a model selection approach with Bayesian or MDL criterion~\citep{wang2017bayesian, proencca2020interpretable, }.

% \subsection{Rule Sets} \label{subsec:related_rulesets}
% \subsubsection{Rule Sets from Decision Trees}
% Any decision tree method can in principle be used as rule induction method, as each path of a decision tree is a decision rule. As no two paths can overlap, each induced rule can be interpreted individually. Numerous decision tree methods exist \citep{kotsiantis2013decision}, where representatives include the classic ones such as CART~\citep{breiman1984classification} and C4.5~\citep{quinlan2014c4}. Nijssen and Fromont proposed a bottom-up way to form decision trees and showed the close connection between decision trees and itemset mining \citep{nijssen2010optimal}.

% The biggest drawback of rules induced by trees is that the tree structure forces rules (i.e., the paths) to share many features, which makes rules longer than necessary, even if the complexity of trees is controlled, e.g., via pruning. Quinlan~\citep{quinlan1987simplifying} proposed a method for simplifying rules induced by trees; however, this causes overlap and an ad-hoc strategy is used to resolve conflicts. 

% \subsubsection{Rule Sets for Binary Targets}
% Many rule set methods for binary classification tasks exist \citep{wang2017bayesian,dash2018boolean}. In particular, if rules are constrained to only cover one class (e.g., the positive class) and always leave the other class to the ``else-rule'', overlapping rules will not cause any conflicts. These methods are clearly not applicable to the more general case of multi-class classification, unless separate ensemble methods are leveraged. Model ensembles are less interpretable than a single model though. For instance, Wang et al.~\citep{wang2017bayesian} suggest to use error-correcting output codes \citep{dietterich1994errorCorrecting}, which requires a distance measure to assign a new data point to the closest code words of the code---this harms explainability. 

% \subsubsection{Rule Sets that allow Overlap}
% Contrary to our approach, which treats the whole rule set as a single probabilistic model that can model any (seen or unseen) data point in a principled way, previous rule set learning methods that allow overlapping rules all require a separate scheme for conflict resolution. unorderer-CN2 \citep{clark1991cn2Improve}, for example, iteratively treats each class label as the ``positive'' class, and uses a sequential covering strategy that removes any covered `positive' data points. In case of conflicts caused by overlap, it uses the union of all involved rules to make the prediction. 
% %Note that although we also use a sequential covering strategy for learning, we do not need a separate strategy for conflict resolution as this is part of our (probabilistic) model (and thus of model selection).

% Bostr{\"o}m~\citep{bostrom2004pruning} proposed to predict the class label of a data point covered by multiple rules using a na\"ive Bayes approach, which calculates the probability of the label given the rules by first calculating the probability of each rule given the label. Following this line, methods using intersecting rules \citep{lindgren2002classification} were proposed, which basically treats each overlap as an intersection (conjunction) of rules; however, this approach potentially creates a huge number of implicit, derived rules and may cause serious overfitting. 

% FURIA~\citep{huhn2009furia} is a fuzzy rule set method, where the conflicts of overlaps are handled by predicting the class label with the highest frequency; hence, FURIA has no probabilistic interpretation. Further, IDS~\citep{lakkaraju2016interpretable} optimizes a linear combination of several scores, which aim for characterizing different aspects of the quality of a rule set, and hence involves extensive hyper-parameter tuning for the ``weights" of this linear combination. To resolve possible conflicts caused by overlap, IDS uses the rule with the highest F1-score. 

% Finally, the very recently proposed DRS~\citep{zhang2020diverseRuleSets} formalizes the problem of rule set learning as a regularized optimization problem that explicitly penalizes large overlaps of rules, and develops a heuristic to choose the regularization parameter to control the degree of penalty without cross-validation. In case of conflicts caused by overlapping rules, DRS suggests to use the most accurate rule. 
%Algorithmically, DRS first reduces the search space by sampling a from all candidate rules, and then adopts a heuristic algorithm to search for the final rule set, which not only learns worse rule sets than our algorithm but also is empirically more time consuming, as shown in Section~\ref{sec:experiments}.

% \paragraph{Other related methods}

% Another related but different approach are rule ensembles, i.e., methods that treat rules as base learners and propose ensemble methods (usually with boosting) for prediction \citep{cohen1999slipper, friedman2008predictive, boley2021better}. However, the interpretability of rule ensembles is not comparable to that of rule sets \citep{dash2018boolean}, as each rule merely becomes a base learner and ``one step'' in a stage-wise optimization process~\citep{friedman2001greedy}. 
%\subsection{Interpretability}
%\subsection{Optimization of Rule Learning and Hardness Results}

