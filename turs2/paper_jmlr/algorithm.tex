\section{Learning Truly Unordered Rules from Data} \label{sec:alg}
Learning decision rules from data is an extremely difficult task; hence, rule learning algorithms have relied on heuristics that try to characterize the ``quality" of individual rules. While recently algorithms that can obtain provably optimal rule lists~\citep{angelino2017learning} and decision trees~\citep{hu2019optimal} are invented, their approach are not applicable to our task due to the following reasons. 

First, our model class (search space) is different as we allow for overlaps among rules, which brings unique challenges even for heuristic algorithms. Second, our model output is probabilistic while the optimal trees/lists algorithms learn rule-based models with non-probabilistic (or just binary) output. Third, our model selection criterion, while requiring no (regularization) hyper-parameters, does not allow efficient search for the global optimum, as like most existing MDL-based approaches~\citep{galbrun2022minimum}. Hence, we also cannot easily apply the branch-and-bounds approaches in the optimal trees/lists algorithms. 

We resort to heuristics as a result, and specifically, we iteratively add the next best rule to the rule set until the MDL-based score is optimized. 

\subsection{Compression ``learning" rate} \label{subsec:learning_rate}
When iteratively searching for the next ``best" rule, defining ``best'' is far from trivial: rule coverage and precision are contradicting factors and typical scores therefore combine those two factors in some---more or less---arbitrary way. 

This issue is further aggravated by the iterative rule learning process, in which the intermediate rule set is evaluated as an \emph{incomplete rule set} in each step. Evaluating incomplete rule sets is a challenging task~\citep{furnkranz2005roc}, mainly because any good score needs to simultaneously consider two aspects: 1) how well do all the rules currently in the rule set describe the already covered instances; and 2) what is the ``potential" for the uncovered instances, in the sense that how well can those uncovered instances be described by rules that might be added later?

To resolve this issue, we consider a greedy heuristic by considering the compression learning rate, i.e., by adding a given rule $S$ to the (potentially incomplete) rule set $\tilde{M}$, how much does the MDL-based score decrease \emph{per extra covered instance}. Formally, 
\begin{equation} \label{eq:learning_rate}
	r(S) = \frac{L(y^n|x^n, \tilde{M}) + L(\tilde{M}) - \left(L(y^n|x^n, \tilde{M}, S) + L(\tilde{M}, S) \right)}{|\tilde{M} \cup S| - |\tilde{M}|}, 
\end{equation}
in which $|\tilde{M}|$ and $|\tilde{M} \cup S|$ respectively denotes the coverage before and after adding $S$ to the (potentially incomplete) rule set $|\tilde{M}|$. 

With the heuristic, our rule learning algorithm becomes a simple iterative algorithm, in which the core is how to find the next best rule. We can keep adding the rule to the rule set as long as the rule has positive $r(S)$: since positive $r(S)$ is equivalent to improvement of the MDL-based score, we do not need separate stopping criterion. We next discuss in detail how to find the next rule in detail. 

\subsection{Finding next rule}
To avoid having to traverse all possible rules when searching for the rule to add to an incomplete rule set, we resort to a common heuristic: we start with an empty rule and gradually refine it by adding literals---also referred to as \emph{growing} a rule \citep{furnkranz2012foundations}. 

For growing the rule, we leverage three individual heuristics, all for resolving different issues. Specifically, we use a \emph{diverse} beam search approach: when growing a rule $S$, we keep the best $w$ rules, each of which has a diverse coverage. 
 Further, we identify a unique algorithm challenge coming along with allowing only ``good" overlaps, and we leverage an auxiliary beam together with a surrogate score to resolve it, which will be discussed in Section~\ref{subsec:auxiliary_beam}. 
 Last, we introduce a local constraint calculated based on the MDL principle, which try to characterize the ``potential" of the left-out part when growing the grow, as discussed in Section~\ref{subsec:local_constraint}. 
 
\subsubsection{Beam search with diverse coverage} \label{subsec:diverse_beam_search}

\noindent \textbf{Motivation.} While we aim to iteratively search for the rule with best the compression learning rate $r(S)$ (Equation~\ref{eq:learning_rate}), it may be too greedy to directly use $r(S)$ to search for the best literal for the next step growth. In other words, $r(S)$, as the heuristic to evaluate the quality of incomplete rule set (and hence also the quality of \emph{complete} rules), may not be suitable to be \emph{directly} used as a quality measure for incomplete rules. 

\noindent \textbf{Method.} Given a potentially incomplete rule $S$, we search all candidate rules $\{S'\}$ that can be obtained by adding a literal to $S$, excluding those (as will be discussed in the following subsection) not satisfying the \emph{local compression constraint}. 

Given a beam width $W$, we categorize all candidate rules $\{S'\}$ into $W$ subgroups according to their coverage: the $w$th subgroup is defined as: 
\begin{equation}
	\{S'\}_w = \{S' \in 	\{S'\}: \frac{|S'|}{|S|} \in \left[\frac{w-1}{W}, \frac{w}{W}\right) \}, \,\,\,\,\,\, w \in \{1, ..., W\}; 
\end{equation}
i.e., the coverage of all candidate growth results $S'$ in $\{S'\}_w$ divided by the coverage of $S$ must be in the interval $[(w-1)/W, w/W)$. 

Next, for each subgroup, we search for the best growth result by optimizing the compression learning rate $r(S')$. In this way, our beam search is diverse regarding the degree of ``patience": when the coverage decreases by a small ratio only, the optimization is ``patient" (by leaving a lot of possibilities for adding more literals); on the other hand, when the coverage decreases by a large ratio, the optimization is greedy (by leaving out little room for further refinement). 

\subsubsection{Auxiliary beam with a surrogate score} \label{subsec:auxiliary_beam}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth, height=0.3\textwidth]{alg1_1_new}
    \includegraphics[width=0.4\textwidth, height=0.3\textwidth]{alg1_2_new}
    \caption{(Left) Simulated data with two overlapping rules: $S_1: X_1 < 0.5$ (outlined in black) and $S_2: 0.5 < X_2 < 1$ (purple). (Right) $S_2$ has  grown to $0.5 < X_2 < 1 \land X_1 < 1.8$, which changes $P(Y|X \in S_2)$ and  resolves the problematic overlap.
    }
    \label{fig:alg1}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth, height=0.3\textwidth]{alg2_1}
    \includegraphics[width=0.4\textwidth, height=0.3\textwidth]{alg2_2}
    \caption{(Left) Simulated data with a rule set containing two rules (black outlines). (Right) Growing a rule to describe the bottom-right instances will create conflicts with existing rules. I.e., adding either $X_1 > 1$ (vertical purple line) or $X_2 < 0.8$ (horizontal purple line) would create a huge overlap that deteriorates the likelihood.}
    \label{fig:alg2}
\end{figure}
Our another novel algorithmic invention is to use an additional auxiliary beam, which is designed to solve the specific algorithmic challenge coming along with our rule set model that only allows overlaps formed by rules with similar probabilistic outputs. 

\noindent \textbf{Motivation.} A rule can only has positive compression learning rate---and thus improve the MDL-based model selection score when it is added to the rule set---if it achieves two goals: 1) it should improve the likelihood of currently uncovered instances (penalized by the approximate-NML normalizing term); and 2) it should \emph{not} deteriorate the goodness-of-fit of the rule set by creating ``bad'' overlaps. These goals can be conflicting though, for two reasons.
   
First, it is not necessarily bad to have overlaps between a rule being grown and the current rule set, because the rule and its probability estimates for the target variable may still change. For example, consider the left plot of Figure~\ref{fig:alg1}. If the current rule set consists of $S_1$ (indicated in black), then adding $S_2$ (in purple) would be problematic: this would strongly deteriorate the likelihood of the instances covered by both rules. However, as we further grow $S_2$, as shown in the right plot, we get $P(Y|S_1) = P(Y|S_2)$ and the problem is solved.

Second, rules already in the rule set may become ``obstacles" to growing a new rule. For example, consider the data and rule set with two rules (in black) in Figure~\ref{fig:alg2}. If we want to grow a rule that covers the bottom-right instances, the existing rules form a blockade: the right plot shows how adding either $X_1 > 1$ or $X_2 < 0.8$ to the empty rule (in purple) would create a large overlap with the existing rules, with significantly different probability estimates.

\noindent \textbf{Surrogate score.}
We propose to use an auxiliary beam together with a surrogate score which informally is the compression learning rate when \emph{ignoring the overlap of between the (potentially incomplete) rule set and the currently growing rule}. 

Formally, given an the ruleset $\tilde{M}$ and a rule $S$, the surrogate score after adding $S$ to $\tilde{M}$, denoted as $R(S)$, is calculated as follows: 1) the probability estimates and likelihood for instances covered by $\tilde{M}$ remains unchanged; 2) the probability estimates, likelihood, and the NML distribution for instances covered by $S$ but \emph{excluding} those covered by $\tilde{M}$, denoted as $S \setminus \tilde{M}$, are calculated based on the ML estimator from $S \setminus \tilde{M}$ while ignoring the overlap between $S$ and $\tilde{M}$. Thus, the surrogate score is defined as 
\begin{equation} \label{eq:surrogate_learning_rate}
	R(S) = \frac{-\log_2 P_M^{apprNML}(Y^{S \setminus \tilde{M}} = y^{S \setminus \tilde{M}} | X^{S \setminus \tilde{M}} = x^{S \setminus \tilde{M}}) + L(S)} {|S \setminus \tilde{M}|},
\end{equation}
in which the numerator can be viewed as the change of the MDL-based score when adding rule $S$ to the rule set $\tilde{M}$ by assuming that $S$ only covers $S \setminus \tilde{M}$. 

By ``ignoring" the overlaps, our surrogate score can search for rule growth candidates that ignores the ``obstacles" from overlaps for now, which create potentially good candidates in the rule growth process. 

\subsubsection{Grow rules with the local compression constraint} \label{subsec:local_constraint}
When growing a rule $S$ by adding a literal and obtaining its growth result $S'$, we essentially leave out part of the cover of $S$ to be potentially covered by future rules. Existing rule learning heuristics often neglect this left-out part but focus only on characterizing the quality of the rule itself. In contrast, we introduce a local constraint that can be viewed as a way of approximately testing whether it is better keep the left-out part, or to leave it for later rules. 

Formally, consider a rule $S$, its growth result $S'$, and the left-out part $S \setminus S' := S_l$, we test whether 
\begin{equation} \label{eq:local_constraint}
	-\log_2 P_S^{apprNML}(y^S|x^S) > -\log_2 P_{S'}^{apprNML}(y^{S'}|x^{S'}) - \log_2 P_{S_l}^{apprNML}(y^{S_l}|x^{S_l}) + L_{split}, 
\end{equation}
where $L_{split}$ is the code length needed to encode the condition that splits $S$ into $S'$ and $S_l$. This requires specifying the variable of the literal and the numeric threshold or the categorical levels (which depends on the variable type). 

Intuitively, this is equivalent to building a depth-one decision tree for all instances covered by $S$, in which the left and right nodes are $S'$ and $S_l$. We then test whether the cover of $S$ can be better compressed by splitting $S$ into its two children, one being its growth result and the other being the left-out part. together with the code length needed to encode the split condition. 

Hence, our additional heuristic is to only allow the growth that satisfy the constraint defined in Equation~\ref{eq:local_constraint}. The rationale is that, if the left-out part can lead to a reasonable compression of its cover when it is reviewed as a separate rule (tree node), it is likely to find some other rules easily later to cover the left-out part, potentially together with other uncovered instances or just partially. 

Finally, note that we use the local constraints both for the beam and the auxiliary beam: when \emph{not} ignoring the overlap, we consider all instances covered by the rule $S$; when ignoring the overlap, we consider instances covered by the rule $S$ minus those covered by the (potentially incomplete) ruleset. 

 
\input{alg_nex_rule}
\subsubsection{Algorithm description}
 After we detailedly discuss the heuristics, we now put all heuristics together and describe in full our algorithm for finding the next rule, of which the pseudo code is provided in Algorithm~\ref{alg:find_next_rule}. 

Assume the potentially empty and potentially incomplete ruleset is $\tilde{M}$, we always start with an \emph{Empty Rule} which contains no literals for its condition, and we initilize the ``rules\_for\_next\_iter" \textbf{[Line 2]} as an array containing the Empty Rule only. 

For each iteration, we initilize a new beam an a new auxiliary beam \textbf{[Line 4-5]} with beam width $W$. The beam keeps the $W$ best rule growth results using the criterion ``compression learning rate" defined in Equation~\ref{eq:learning_rate}; in contrast, the auxiliary beam keeps the $W$ best rule growth results using the ``surrogate" score by ignoring the rule's overlap with $\tilde{M}$, as discussed in detail in Section~\ref{subsec:auxiliary_beam}. 

Next, we use every rule in the ``rules\_for\_next\_iter" array as a ``base" for growing \textbf{[Line 6-16]}. Specifically, given a rule, we first generate its candidate growth \emph{by adding one  literal only} \textbf{[Line 6]}. That is, we go over all feature variables in the dataset, and for each variable, we generate candidate literals with numeric thresholds (often quantiles) or with categorical levels, based on the variable type. For numeric variables, we need to specify the granularity of search, which can be considered as a hyper-parameter. 

Further, we categorize the candidates by their coverage (for the beam) and their coverage excluding the instances already covered by $\tilde{M}$ (for the auxiliary beam) \textbf{[Line 8 \& 11]}. We then search for the best candidate in the beam and the auxiliary beam, using $r(.)$ and $R(.)$ (Section~\ref{subsec:auxiliary_beam}) as the criterion respectively, together with the local constraint (Section~\ref{subsec:local_constraint}) \textbf{[Line 9-10 \& 11-12]}. Note that the ``diverse coverage heuristic" is only used for growing individual rules; however, when updating ``beam" and ``auxiliary\_beam", we do not take into consideration the coverage heuristic, as the growth candidates from different rules cannot be categorized \textbf{[Line 14-15]}. 

To check whether the growing process should be stopped after this iteration, a greedy approach would be to stop when the ``beam" of this iteration does not produces rules with better compression learning rate $r(S)$ than the previous iteration's ``beam". Yet, we took a less greedy approach that we only stop when this is $K_{stop}$-th time in a row that both beams (i.e., the beam and the auxiliary beam) produce ``worse" rules than the previous beams. Note that the criterion of being ``worse" for the auxiliary beam is the surrogate score \textbf{[Line 17]}. Specifically, $K_{stop}$ is a user-defined parameter that controls the ``budget" of the algorithm. In practice, we find $K_{stop} = 5$ sufficient. 

Last, if the stopping criterion is not met, we update ``All\_candidate\_rules" and \\
``rules\_for\_next\_iter" \textbf{[Line 22-23]}, and continue to the next iteration \textbf{[Line 3]}. The former is the pool we use for finally selecting the next best rule to be potentially added to $\tilde{M}$. On the other hand, if the stopping criterion is met, we return the rule $S$ among ``All\_candidate\_rules" with the best (largest) compression learning rate $r(.)$ \textbf{[Line 19-20]}. 






